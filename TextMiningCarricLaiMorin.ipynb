{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextMining_Carric_Lai_Morin.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbUL5T8hgyd8"
      },
      "source": [
        "# **PROJET TEXT MINING** - *Option 6*\r\n",
        "---\r\n",
        "\r\n",
        "Quentin Carric - Xavier Lai - Alexis Morin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIwC-MbzV_uY"
      },
      "source": [
        "## **I - Scrapping**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itWIc6T9VC7R"
      },
      "source": [
        "from lxml import html\r\n",
        "import requests\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "from urllib.error import HTTPError, URLError\r\n",
        "import sys\r\n",
        "from progress.bar import IncrementalBar\r\n",
        "import json\r\n",
        "\r\n",
        "\r\n",
        "'''\r\n",
        "def get_content(url):\r\n",
        "    try:\r\n",
        "        resp = requests.get(url)\r\n",
        "    except (HTTPError, URLError) as error:\r\n",
        "        sys.exit(error)\r\n",
        "    return BeautifulSoup(resp.content, \"html.parser\")\r\n",
        "\r\n",
        "def get_game_url_from_page(url):\r\n",
        "    url_game_list = []\r\n",
        "    page_content_bs = get_content(url)\r\n",
        "    game_tag_a_list = page_content_bs.find_all('a', {\"class\" : \"product-image-link\"})\r\n",
        "    for game_url in game_tag_a_list:\r\n",
        "        url_game_list.append(\"https://www.micromania.fr\" + game_url.attrs['href'])\r\n",
        "    return url_game_list\r\n",
        "\r\n",
        "\r\n",
        "def get_game_infos_dict(game_url):\r\n",
        "    page_content_bs = get_content(game_url)\r\n",
        "    try:\r\n",
        "        game_description = page_content_bs.find('article', {\"class\" : \"product-details__description\"}).find('p').get_text()\r\n",
        "        game_type = page_content_bs.find('span', {'class' : 'attribute-value product-details__atr-value col-7 col-md-8'}).get_text()\r\n",
        "    except AttributeError:\r\n",
        "        return {\"description\" : None, \"type\" : None}\r\n",
        "    return {\"description\" : game_description, \"type\" : game_type}\r\n",
        "\r\n",
        "\r\n",
        "def extrat_all_games(number_page):\r\n",
        "    start_value = 0\r\n",
        "    desc_and_type_list = []\r\n",
        "    for i in range(1,number_page):\r\n",
        "        url = f'https://www.micromania.fr/jeux.html?start={start_value}&sz=48&page={i}'\r\n",
        "        game_url_list = get_game_url_from_page(url)\r\n",
        "        bar = IncrementalBar('scrap video games', max = len(game_url_list))\r\n",
        "        print(f\"\\npage {i}\") \r\n",
        "        for game_url in game_url_list:\r\n",
        "            bar.next()\r\n",
        "            \r\n",
        "            desc_and_type_list.append(get_game_infos_dict(game_url))\r\n",
        "        start_value += 48\r\n",
        "        bar.finish()\r\n",
        "    return desc_and_type_list\r\n",
        "\r\n",
        "result = extrat_all_games(30)\r\n",
        "with open('data_projet_textmining', 'w') as file:\r\n",
        "    json.dump(result, file)\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLIxYmCbaIUh"
      },
      "source": [
        "On convertit ensuite notre fichier *json* en un dataframe pandas en regroupant les modalités de notre **Y**.\r\n",
        "\r\n",
        "**Y** aura donc 3 modalités : \"Sport et conduite\", \"Action & Guerre\" et \"Autres\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghnTardPVssO",
        "outputId": "264ce520-bb56-401c-b02d-98a9de667900"
      },
      "source": [
        "!pip install progress"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting progress\n",
            "  Downloading https://files.pythonhosted.org/packages/38/ef/2e887b3d2b248916fc2121889ce68af8a16aaddbe82f9ae6533c24ff0d2b/progress-1.5.tar.gz\n",
            "Building wheels for collected packages: progress\n",
            "  Building wheel for progress (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for progress: filename=progress-1.5-cp36-none-any.whl size=8075 sha256=90e44487c7de44b7762bd3d141fc289b5a1bb6e6e0177729a7458e997ea6f4b8\n",
            "  Stored in directory: /root/.cache/pip/wheels/6c/c8/80/32a294e3041f006c661838c05a411c7b7ffc60ff939d14e116\n",
            "Successfully built progress\n",
            "Installing collected packages: progress\n",
            "Successfully installed progress-1.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wHNYvXNVUef",
        "outputId": "977469d9-76a9-463a-df0a-c01e4aecabdc"
      },
      "source": [
        "import json\r\n",
        "import pandas as pd\r\n",
        "from lxml import html\r\n",
        "import requests\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "from urllib.error import HTTPError, URLError\r\n",
        "import sys\r\n",
        "from progress.bar import IncrementalBar\r\n",
        "import json\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "dico_type = {\r\n",
        "    \"Action / aventure\" :\"Action & Guerre\",\r\n",
        "    \"Jeu de rôle\" : \"Autres\",\r\n",
        "    \"Sport\" : \"Sport & Conduite\",\r\n",
        "    \"Football\" : \"Sport & Conduite\",\r\n",
        "    \"Danse / musique\" : \"Sport & Conduite\",\r\n",
        "    \"Action\" : \"Action & Guerre\",\r\n",
        "    \"Course / arcade\" : \"Sport & Conduite\",\r\n",
        "    \"Gestion\" : \"Autres\",\r\n",
        "    \"Wargame\" : \"Action & Guerre\",\r\n",
        "    \"Baston / combat\" : \"Action & Guerre\",\r\n",
        "    \"Activités récréatives\" : \"Autres\",\r\n",
        "    \"Plate-forme\" : \"Autres\",\r\n",
        "    \"Tir\" : \"Action & Guerre\",\r\n",
        "    \"Console\" : \"Autres\",\r\n",
        "    \"Aventure\" : \"Action & Guerre\",\r\n",
        "    \"Stratégie temps réel\" : \"Action & Guerre\",\r\n",
        "    \"Infiltration\" : \"Action & Guerre\",\r\n",
        "    \"Rallye\" : \"Sport & Conduite\",\r\n",
        "    \"Basket\" : \"Sport & Conduite\",\r\n",
        "    \"Réflexion\" : \"Autres\",\r\n",
        "    \"Quake-like\" : \"Action & Guerre\",\r\n",
        "    \"Action / réflexion\" : \"Action & Guerre\",\r\n",
        "    \"Accessoire\" : \"Autres\",\r\n",
        "    \"Beat'em all\" : \"Action & Guerre\",\r\n",
        "    \"Shoot'em up\" : \"Action & Guerre\",\r\n",
        "    \"Course / simulation\" : \"Sport & Conduite\",\r\n",
        "    \"Simulation de vol\" : \"Sport & Conduite\",\r\n",
        "    \"Shoot tactique\" : \"Action & Guerre\",\r\n",
        "}\r\n",
        "\r\n",
        "description_list=[]\r\n",
        "type_list=[]\r\n",
        "with open(\"data_projet_textmining.json\") as json_file:\r\n",
        "    data = json.load(json_file)\r\n",
        "    description_list = []\r\n",
        "    for game_dict in data :\r\n",
        "        data_is_valid = (\r\n",
        "            game_dict['type'] is not None and \r\n",
        "            game_dict['description'].strip() !='' and \r\n",
        "            game_dict['description'] not in description_list\r\n",
        "            )\r\n",
        "        if data_is_valid:\r\n",
        "            description_list.append(game_dict['description'].strip())\r\n",
        "            type_list.append(game_dict['type'].replace('\\n', \"\").strip())\r\n",
        "\r\n",
        "df = pd.DataFrame({'description': description_list, 'type': type_list})\r\n",
        "df = df.replace(dico_type)\r\n",
        "df[\"type\"] = df[\"type\"].astype(\"category\")\r\n",
        "print(df)\r\n",
        "print(df.groupby('type').nunique())\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                           description              type\n",
            "0    Bowser a encore fait des siennes... et cette f...   Action & Guerre\n",
            "1    Le jeu de tir de référence est de retour avec ...            Autres\n",
            "2    Vous avez toujours une bonne excuse pour ne pa...  Sport & Conduite\n",
            "3    Rassemblez-vous et faites passer le message : ...  Sport & Conduite\n",
            "4    Véritable phénomène international, la saga ...   Action & Guerre\n",
            "..                                                 ...               ...\n",
            "601  Smart Four Connected est un jeu de puzzle élec...            Autres\n",
            "602  Pour la 1ere fois en France, découvrez le stre...  Sport & Conduite\n",
            "603  « Les légendes éorzéennes racontent que lorsqu...            Autres\n",
            "604  L’édition spéciale d’Harvest Moon : Lumière d’...            Autres\n",
            "605  Dans cette nouvelle aventure, Dr. Eggman, touj...            Autres\n",
            "\n",
            "[606 rows x 2 columns]\n",
            "                  description\n",
            "type                         \n",
            "Action & Guerre           289\n",
            "Autres                    197\n",
            "Sport & Conduite          116\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zkCP91Xg9hk"
      },
      "source": [
        "Nous avons scrappé les descriptions des jeux vidéos ainsi que la catégorie dans laquelle ils appartiennent (source : https://www.micromania.fr/jeux.html)\r\n",
        "\r\n",
        "Nous avons donc en *features* les descriptions , et la variable à expliquer est la catégorie du jeu. Le but est de construire un algortihme qui permettra de prédire la catégorie d'un jeu en regardant uniquement sa description."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptssfqschlo7"
      },
      "source": [
        "## **II - Traitements de données**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfMtJse7i67e",
        "outputId": "007e87f2-3b69-4046-c4e5-c9d90500b1c1"
      },
      "source": [
        "!pip install nltk"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NX3egS3Sf7l1",
        "outputId": "79b1b190-2948-43a4-f2d3-3ab9a1c55914"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from string import punctuation\r\n",
        "import nltk\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\r\n",
        "from nltk.corpus import stopwords\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from wordcloud import WordCloud, STOPWORDS \r\n",
        "import pickle\r\n",
        "from nltk.stem import PorterStemmer\r\n",
        "from sklearn.feature_extraction.text import TfidfTransformer,TfidfVectorizer, CountVectorizer\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "from sklearn.model_selection import GridSearchCV\r\n",
        "from sklearn.model_selection import validation_curve\r\n",
        "from sklearn.pipeline import make_pipeline\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import scipy.sparse\r\n",
        "\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('stopwords')\r\n",
        "\r\n",
        "# local functions\r\n",
        "\r\n",
        "def remove_ponctuation(s):\r\n",
        "    global punctuation\r\n",
        "    for p in punctuation:\r\n",
        "        s = s.replace(p, '')\r\n",
        "    return s\r\n",
        "\r\n",
        "def identify_tokens(row):\r\n",
        "    review = row\r\n",
        "    tokens = nltk.word_tokenize(review)\r\n",
        "    # taken only words (not punctuation)\r\n",
        "    token_words = [w for w in tokens if w.isalpha()]\r\n",
        "    return token_words\r\n",
        "\r\n",
        "def stem_list(row):\r\n",
        "    my_list = row\r\n",
        "    stemmed_list = [stemming.stem(word) for word in my_list]\r\n",
        "    return (stemmed_list)\r\n",
        "\r\n",
        "def remove_stops(row):\r\n",
        "    my_list = row\r\n",
        "    meaningful_words = [w for w in my_list if not w in stops]\r\n",
        "    return (meaningful_words)\r\n",
        "\r\n",
        "def rejoin_words(row):\r\n",
        "    my_list = row\r\n",
        "    joined_words = ( \" \".join(my_list))\r\n",
        "    return joined_words\r\n",
        "\r\n",
        "def export_predictions(nom,pred):\r\n",
        "    dtest = pd.read_json(Path_data+\"test.json\")\r\n",
        "    dtest['Category'] = pred\r\n",
        "    soumission = dtest[[\"Id\",\"Category\"]]\r\n",
        "    soumission.to_csv(nom+\".csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIUAQUtZaEWG"
      },
      "source": [
        "### Mise en place de la validation croisée : Méthode Hold out"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQqmdxG5WRhZ"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "train, test = train_test_split(df, test_size=0.2,random_state = 42)\r\n",
        "X_train = train.description\r\n",
        "X_test = test.description\r\n",
        "Y_train = train.type\r\n",
        "Y_test = test.type"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMkXhGAoiSEo"
      },
      "source": [
        "## Traitement de texte\r\n",
        "\r\n",
        "# A) Lower\r\n",
        "X_train = X_train.str.lower()\r\n",
        "X_test = X_test.str.lower()\r\n",
        "\r\n",
        "# B) Remove ponctuation\r\n",
        "X_train = X_train.map(remove_ponctuation)\r\n",
        "X_test = X_test.map(remove_ponctuation)\r\n",
        "\r\n",
        "# C) Tokenisation+ stemmisation\r\n",
        "X_train = X_train.apply(identify_tokens)\r\n",
        "stemming = PorterStemmer()\r\n",
        "X_train = X_train.apply(stem_list)\r\n",
        "\r\n",
        "X_test = X_test.apply(identify_tokens)\r\n",
        "X_test = X_test.apply(stem_list)\r\n",
        "\r\n",
        "# D) Keep only meaningful stem and join\r\n",
        "stops = set(stopwords.words(\"french\")) \r\n",
        "X_train = X_train.apply(remove_stops)\r\n",
        "X_train = X_train.apply(rejoin_words)\r\n",
        "\r\n",
        "X_test = X_test.apply(remove_stops)\r\n",
        "X_test = X_test.apply(rejoin_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17GUf_9ViQUd"
      },
      "source": [
        "## **III - Featuring (TF-ID)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgx8AKgGX9LW",
        "outputId": "6527e231-f5a1-44c4-8007-0950a6786bbe"
      },
      "source": [
        "transformer = TfidfVectorizer().fit(X_train.values)\r\n",
        "print(\"NB features: %d\" %(len(transformer.vocabulary_)))\r\n",
        "x_train_tfID = transformer.transform(X_train.values)\r\n",
        "x_test_tfID = transformer.transform(X_test.values)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NB features: 5133\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGU72EGTYJu9"
      },
      "source": [
        "## **IV - Learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrKM1g8tZZpR"
      },
      "source": [
        "### A) Régression logistique multinomiale"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncYATHc6YI55",
        "outputId": "72746cc8-712e-4b82-ab16-6d1fdee09ae8"
      },
      "source": [
        "logistic = LogisticRegression()\r\n",
        "logistic.fit(x_train_tfID, Y_train)\r\n",
        "predictions = logistic.predict(x_test_tfID)\r\n",
        "predictions[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Autres', 'Action & Guerre', 'Action & Guerre', 'Action & Guerre',\n",
              "       'Action & Guerre', 'Action & Guerre', 'Action & Guerre',\n",
              "       'Action & Guerre', 'Action & Guerre', 'Action & Guerre'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uASbwcMAYPvc",
        "outputId": "1e7d3d69-64d2-4160-dd4a-066568fd0fff"
      },
      "source": [
        "logistic.score(x_test_tfID,Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5819672131147541"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iIDR-jdbmBo"
      },
      "source": [
        "### B) Random forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-E7jPO1boK-"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "from sklearn.model_selection import GridSearchCV\r\n",
        "\r\n",
        "def gridSearch_Pipeline(algo_pipeline,dico_params, nb_cv = 5,x_train = x_train_tfID,y_train=Y_train, x_test=x_test_tfID, y_test=Y_test):\r\n",
        "    grid = GridSearchCV(algo_pipeline,dico_params,cv = nb_cv)\r\n",
        "    #grid.get_params() pour voir les paramètres possibles\r\n",
        "    grid.fit(x_train,y_train)\r\n",
        "    print(\"\\n\",\"-\"*40,\"\\nBest parameter (CV score=%0.3f):\"%grid.best_score_)\r\n",
        "    print(grid.best_params_)\r\n",
        "    print(\"{}\\nEstimation de l'Accuracy :\\n{}\".\\\r\n",
        "          format('*'*30,grid.score(x_test,y_test)))\r\n",
        "    \r\n",
        "\r\n",
        "    pred_algo = grid.predict_proba(x_test)[:,1]\r\n",
        "    accuracy_algo = grid.score(x_test,y_test).round(3)\r\n",
        "    \r\n",
        "    return grid,pred_algo,accuracy_algo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXTImytscvhq",
        "outputId": "d08f2411-83e2-49f5-8219-51367bf9d2ec"
      },
      "source": [
        "from sklearn.pipeline import Pipeline, make_pipeline\r\n",
        "\r\n",
        "RF_pipeline = make_pipeline(\r\n",
        "    RandomForestClassifier(random_state=0,n_jobs = -1))\r\n",
        "\r\n",
        "mtry = np.arange(1500,2500,25)\r\n",
        "params={\r\n",
        "        \"randomforestclassifier__max_features\": mtry,\r\n",
        "        }\r\n",
        "\r\n",
        "grid, pred_rf, accur_rf = gridSearch_Pipeline(RF_pipeline,params)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " ---------------------------------------- \n",
            "Best parameter (CV score=0.562):\n",
            "{'randomforestclassifier__max_features': 1850}\n",
            "******************************\n",
            "Estimation de l'Accuracy :\n",
            "0.5573770491803278\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}